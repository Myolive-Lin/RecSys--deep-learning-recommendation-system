{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4133, 0.1791, 0.4077], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        \"\"\"\n",
    "        初始化 Embedding 层。\n",
    "        :param input_dim: 输入特征的维度。\n",
    "        :param emb_dim: 嵌入的维度。\n",
    "        \"\"\"\n",
    "        super(Embedding, self).__init__()\n",
    "        self.input_matrix = nn.Parameter(torch.randn(input_dim,emb_dim)) #输入特征的嵌入矩阵\n",
    "        self.output_matrix = nn.Parameter(torch.randn(emb_dim,input_dim)) #输出特征的嵌入矩阵\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        :param x: 输入张量，形状为 (batch_size, input_dim)。\n",
    "        :return: 返回样本与整个词汇表的点积，形状为 (batch_size, input_dim)。\n",
    "        \"\"\"\n",
    "        embedding = torch.matmul(x,self.input_matrix) #获取输入特征的嵌入表示\n",
    "        product = torch.matmul(embedding, self.output_matrix) #计算输入特征与整个词汇表的点积\n",
    "        return torch.softmax(product,-1) #对点积进行softmax归一化 (batch_size, input_dim)\n",
    "\n",
    "        \n",
    "\n",
    "# 定义超参数\n",
    "vocab_size = 3  # 词汇表大小\n",
    "embedding_dim = 2  # 嵌入维度\n",
    "batch_size = 2  # 批量大小\n",
    "seq_len = 3  # 序列长度\n",
    "epochs = 10  # 训练轮数\n",
    "\n",
    "# 实例化模型\n",
    "model = Embedding(input_dim=vocab_size, emb_dim=embedding_dim)\n",
    "model(torch.tensor([1,0,0],dtype= torch.float))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.6127016653792583, 2: 0.726138721247417, 3: 0.7763932022500211, 4: 0.8063508326896291, 5: 0.8267949192431123}\n",
      "Original words: [1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5]\n",
      "Subsampled words: [2, 4, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def subsample(words, freq_threshold=1e-2):\n",
    "    \"\"\"\n",
    "    子采样函数\n",
    "    :param words: 单词索引列表\n",
    "    :param freq_threshold: 频率阈值\n",
    "    :return: 子采样后的单词索引列表\n",
    "    \"\"\"\n",
    "    # 使用 collections.Counter 统计每个单词的频率\n",
    "    word_counts = collections.Counter(words)\n",
    "    total_words = len(words)\n",
    "    \n",
    "    # 计算每个单词的频率\n",
    "    word_freq = {word: count / total_words for word, count in word_counts.items()}\n",
    "    \n",
    "    # 计算丢弃概率\n",
    "    drop_prob = {word: 1 - np.sqrt(freq_threshold / freq) for word, freq in word_freq.items()}\n",
    "    \n",
    "    # 子采样\n",
    "    subsampled_words = [word for word in words if np.random.random() >  drop_prob[word]]\n",
    "    print(drop_prob)\n",
    "    \n",
    "    return subsampled_words\n",
    "\n",
    "# 示例：子采样\n",
    "words = [1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5]  # 假设的单词索引列表\n",
    "subsampled_words = subsample(words)\n",
    "print(\"Original words:\", words)\n",
    "print(\"Subsampled words:\", subsampled_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5] [0.08981959121294449, 0.1510579445410381, 0.2047443920227003, 0.2540481681203067, 0.30032990410301025]\n",
      "Original words: [1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5]\n",
      "Unigram distribution: {1: 0.08981959121294449, 2: 0.1510579445410381, 3: 0.2047443920227003, 4: 0.2540481681203067, 5: 0.30032990410301025}\n",
      "Sampled words: [4 3 5]\n"
     ]
    }
   ],
   "source": [
    "def get_unigram_distribution(words,power):\n",
    "    \"\"\"\n",
    "    子采样函数\n",
    "    :param words: 单词索引列表\n",
    "    :param power: 次方\n",
    "    :return: 采样分布\n",
    "    \"\"\"\n",
    "    # 使用 collections.Counter 统计每个单词的频率\n",
    "    word_counts = collections.Counter(words)\n",
    "    total_words = len(words)\n",
    "    \n",
    "    # 计算每个单词的频率\n",
    "    word_freq_power = {word: np.power(count / total_words, power) for word, count in word_counts.items()}\n",
    "    word_sum = sum(word_freq_power.values())\n",
    "\n",
    "    # 计算每个单词的概率\n",
    "    word_prob = {word: freq_power / word_sum for word, freq_power in word_freq_power.items()}\n",
    "\n",
    "    return word_prob\n",
    "\n",
    "def get_negative_samples(word_prob, k = 5):\n",
    "    \"\"\"\n",
    "    从采样分布中随机抽取负样本\n",
    "    :param word_prob: 采样分布, 字典形式, key为单词索引, value为概率\n",
    "    :param words: 单词索引列表\n",
    "    :param num_samples: 每个单词的负样本数量\n",
    "    :return: 负样本列表\n",
    "    \"\"\"\n",
    "    \n",
    "    # 从采样分布中随机抽取负样本\n",
    "    word = list(word_prob.keys())\n",
    "    prob = list(word_prob.values())\n",
    "\n",
    "    print(word,prob)\n",
    "    negative_samples =  np.random.choice(word, size=k, p=prob)\n",
    "    return negative_samples\n",
    "\n",
    "words = [1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5]  # 假设的单词索引列表\n",
    "power = 0.75  # 次方\n",
    "\n",
    "# 计算 unigram 分布\n",
    "word_prob = get_unigram_distribution(words, power)\n",
    "\n",
    "# 采样\n",
    "num_samples = 3  # 需要采样的单词数量\n",
    "samples = get_negative_samples(word_prob, num_samples)\n",
    "\n",
    "print(\"Original words:\", words)\n",
    "print(\"Unigram distribution:\", word_prob)\n",
    "print(\"Sampled words:\", samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
